# Search-Based Software Engineering Course WS21/22

For now, the Jupyter notebooks are provided as is, and you will have to download and open them in Jupyter. Here's [how to install jupyter notebook on your machine](https://www.dataquest.io/blog/jupyter-notebook-tutorial/).


## Chapter 1: Random and Local Search

The first chapter covers the coding examples from the first two weeks, on basic random search and local search algorithms.

## Chapter 2: Evolutionary Search (Part 1)

This chapter covers basic evolutionary strategies and genetic algorithms.

## Chapter 3: Multi-Objective Optimisation (Part 1)

This chapter covers the basics of Pareto optimality, NSGA-II, and comparison
of multi-objective search algorithms.

## Chapter 4: Multi-Objective Optimisation (Part 2)

This chapter covers several alternative multi-objective search algorithms:
A random baseline, PAES, SPEA2, TwoArchives, and SMS-EMOA.

## Chapter 5: Evolutionary Search (Part 2)

This chapter looks into the various search operators of a genetic algorithm:
Survivor selection, parent selection, crossover, mutation, and the
population itself.

## Chapter 6: Search-based Test Generation (Part 1)

This chapter looks at how the problem of test input generation can be cast
as a search problem, and how to automatically instrument programs for
fitness generation. 

## Chapter 7: Search-based Test Generation (Part 2)

This chapter continues whole test suite generation, and then moves on to
many objective optimisation for test generation.

## Chapter 8: Genetic Programming (Part 1)

This chapter introduces classic genetic programming for scenarios assuming
type closure, and applies this to symbolic regression.

## Chapter 9: Genetic Programming (Part 2)

This chapter continues genetic programming by optimising spectrum-based
fault localisation, grammatical evolution, and automated program repair.

## Chapter 10: Parameter Tuning and Parameter Control

This chapter considers how to choose values for the many parameters that we
have introduced in our evolutionary algorithms, how to optimise these
values, and how to adapt them to new problems.

## Chapter 11: Advanced Evolutionary Algorithms

This chapter considers several advanced variants of the evolutionary
algorithms we have discussed in previous chapter: Memetic algorithms combine
global and local search; island model GAs divide the population of a GA into
independent subpopulations; estimation of distribution algorithms try to
explicitly optimise the probability distribution that is otherwise
implicitly represented by the population; differential evolution uses novel
search operators unlike the ones we have used in standard GAs; hyper 
heuristics try to combine different heuristics to adapt to the problem at hand.

## Chapter 12: Swarm Optimisation

This chapter briefly introduces two swarm optimisation techniques: Ant colony optimisation
tries to imitate the stigmergic communication of ants for the purpose of
optimisation. Particle swarm optimisation simulates the swarm behaviour of
birds or fish.
